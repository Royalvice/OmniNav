# OmniNav 需求规格说明书

## 1. 项目愿景与目标

### 1.1 项目背景
随着具身智能（Embodied AI）技术的快速发展，对于能够在复杂、动态环境中进行自主导航和决策的智能体的需求日益增长。现有的仿真平台往往难以兼顾高性能物理仿真、丰富的场景资产导入、以及对前沿算法（如 VLA、VLN）的原生支持。

### 1.2 核心愿景
OmniNav 旨在构建一个**面向下一代具身智能（Embodied AI）的上游应用仿真平台**。它将作为一个通用的、高性能的、模块化的中间件，连接底层的物理引擎（Genesis）与上层的智能算法应用，填补从算法原型到真机部署（Sim2Real）之间的鸿沟。

### 1.3 核心目标
*   **多模态感知与仿真**：提供高保真的视觉（RGB-D）、激光雷达（Lidar）和其他本体感知传感器的仿真。
*   **算法验证加速器**：支持从传统导航算法到最新的端到端神经网络策略的快速验证与迭代。
*   **虚实融合桥梁**：通过高精度的场景重建和物理参数标定，实现 Real2Sim 和 Sim2Real 的无缝切换。
*   **标准化与可扩展性**：定义清晰的接口规范，支持自定义机器人模型、传感器配置及评测任务。

---

## 2. 系统架构需求

### 2.1 分层架构设计
为了保证系统的灵活性和可维护性，OmniNav 必须采用严格的分层架构设计。各层之间需定义清晰的交互接口，严禁跨层耦合。

#### 2.1.1 核心层 (Core Layer)
*   **仿真引擎适配**：底层必须基于 **Genesis** 物理引擎构建，充分利用其高性能并行仿真能力。
*   **配置管理**：所有系统配置（环境、机器人、任务、算法）必须统一使用 **OmegaConf** 和 **Hydra** 进行管理，支持命令行动态覆盖和多环境配置组合。
*   **注册机制**：实现全系统的组件注册机制（Registry），支持机器人、传感器、控制器、任务、度量指标的动态加载与实例化。

#### 2.1.2 资产与场景层 (Asset & Scene Layer)
*   **多格式资产支持**：必须原生支持多种主流 3D 资产格式的导入，包括但不限于 **USD (Universal Scene Description)**、GLB/gLTF、OBJ、URDF、MJCF。
*   **程序化场景生成 (Procedural Generation)**：
    *   支持基于规则的场景自动生成，能够根据难度参数自动布局墙体、地面、障碍物等。
    *   支持随机化场景元素（纹理、光照、物体位置），以增强训练算法的鲁棒性。
*   **场景复杂度评估**：提供自动评估场景复杂度的工具，基于障碍物密度、路径曲率、视觉遮挡度等指标量化场景难度。

#### 2.1.3 机器人与感知层 (Robot & Perception Layer)
*   **模块化机器人定义**：所有机器人必须继承自统一基类，支持通过配置文件定义其动力学参数、外观及初始状态。
*   **传感器动态挂载**：必须支持传感器的“可插拔”设计。传感器（如 2D/3D Lidar、RGB-D 相机、IMU）应作为独立模块，通过配置文件动态挂载到机器人的任意 Link 上。
*   **多类型机器人支持**：
    *   **四足机器人**：原生支持宇树科技（Unitree）Go2 等四足机器人，需包含精细的关节动力学模型。
    *   **轮式机器人**：支持 Unitree Go2w 等全向轮/差速轮机器人。
    *   **扩展支持**：架构需预留接口，支持未来扩展至无人机（Drone）、机械臂、以及多机器人编队系统。

#### 2.1.4 算法接口层 (Algorithm Interface Layer)
*   **统一交互接口**：定义标准化的 `Observation`（观测）和 `Action`（动作）空间接口。
    *   **输入**：包含传感器数据（图像、点云）、本体状态（关节角、速度）、任务指令（语言、目标点）。
    *   **输出**：支持统一的控制指令格式，如速度指令 `cmd_vel` (vx, vy, wz) 或底层关节控制信号。
    *   **语言指令预留**：接口必须预留自然语言指令（Language Instruction）字段，以支持 VLA/VLN 等图文导航任务。
*   **并行环境支持**：必须充分利用 Genesis 的并行仿真能力，支持批量化环境（Vectorized Environments）的数据采集与训练，接口需支持 `(num_envs, ...)` 维度的张量数据流。

#### 2.1.5 评测与任务层 (Evaluation & Task Layer)
*   **可插拔任务定义**：支持通过配置文件定义不同的评测任务。
*   **标准化评测指标**：内置通用的导航评测指标计算模块。

---

## 3. 功能需求详情

### 3.1 核心仿真功能
1.  **高保真物理仿真**：基于 Genesis 提供刚体动力学、碰撞检测、关节约束的精确计算。
2.  **高性能渲染**：支持光线追踪（Ray Tracing）和光栅化（Rasterization）两种渲染模式，满足视觉算法对逼真度和训练速度的不同需求。

### 3.2 导航与算法支持
1.  **传统规划算法验证**：支持集成 A*、Dijkstra、DWA (Dynamic Window Approach)、TEB 等经典路径规划与避障算法。
2.  **学习型算法验证**：支持强化学习（RL）、模仿学习（IL）等算法的数据接口，支持 Sim2Real 的训练流程。
3.  **VLA/VLN 支持**：为视觉语言动作模型（Vision-Language-Action）和视觉语言导航（Vision-Language Navigation）提供必要的环境交互接口（图像+文本输入，动作输出）。
4.  **算法热插拔**：用户应能在不重启仿真器的情况下，通过配置切换不同的导航算法进行对比测试。

### 3.3 评测系统
#### 3.3.1 任务类型
*   **PointNav (点到点导航)**：在无地图或已知地图环境下，从随机起点导航至指定坐标终点。
*   **ObjectNav (目标导向导航)**：根据语义指令（如“找到沙发”），在环境中搜索并接近特定类别的物体。
*   **AreaNav (区域覆盖)**：要求机器人探索并覆盖指定区域或整个环境。

#### 3.3.2 评价指标
*   **成功率 (Success Rate, SR)**：任务成功完成的比例。
*   **路径加权成功率 (SPL)**：综合考虑成功率与路径长度的指标，衡量导航效率。
*   **碰撞数据 (Collision Metrics)**：记录碰撞次数、碰撞强度。
*   **时间效率 (Time Efficiency)**：完成任务所需的步数或仿真时间。

### 3.4 虚实融合 (Real-to-Sim & Sim-to-Real)
1.  **场景重建 (Scene Reconstruction)**：
    *   支持导入真实场景的 3D 扫描模型。
    *   **[高级]** 支持基于 **Gaussian Splatting** 或 **NeRF** 的场景重建数据导入，以此构建与真实世界视觉表现高度一致的虚拟环境。
2.  **轨迹重放 (Trajectory Replay)**：支持读取真机录制的传感器数据（ROSbag 或自定义格式）和关节状态序列，在仿真器中进行开环回放，用于对比仿真与真机的行为差异。
3.  **模型标定 (System Identification)**：提供工具流，利用真机采集的数据，通过优化算法自动校准仿真环境中的物理参数（如摩擦系数、电机阻尼、质量分布），以缩小 Sim-to-Real Gap。

### 3.5 外部接口与生态
1.  **ROS2 通信接口**：
    *   提供基于 `humble` 版本的 ROS2 Bridge。
    *   支持双向通信：仿真器发布传感器话题 (`/scan`, `/image_raw`, `/odom`, `/tf`)，订阅控制话题 (`/cmd_vel`)。
    *   支持 ROS2 Time (`/clock`) 同步，确保仿真时间与 ROS 时间对齐。
    *   支持与 ROS2 Navigation Stack (Nav2) 对接。
2.  **文档与部署**：
    *   提供符合行业标准的开发者文档。
    *   可以通过 GitHub Pages 自动部署文档网站。

---

## 4. 优先级与路线图 (Roadmap Priorities)

为了确保项目按时交付并逐步迭代，我们将需求功能划分为不同的优先级阶段。

### 第一阶段：核心平台与基础导航 (P0 - Critical)
**目标**：构建最小可行产品 (MVP)，实现基本的导航仿真闭环。
1.  **基础架构**：完成 Core Layer、Robot Layer、Sensor Layer 的开发。
2.  **机器人支持**：实现 Unitree Go2 (四足) 和 Go2w (轮式) 的完整支持及传感器挂载。
3.  **导航验证**：打通 `Observation` -> `Algorithm` -> `Control` 的链路，支持简单的传统导航算法验证。
4.  **基本场景**：支持基础的平面场景及简单的障碍物生成。

### 第二阶段：感知增强与 ROS2 集成 (P0 - Critical)
**目标**：完善感知能力，对接机器人主流生态。
1.  **传感器完善**：实现高精度的 RGB-D 相机和 2D/3D Lidar 仿真。
2.  **ROS2 Bridge**：实现完整的 ROS2 通信桥接，支持 Topic 发布订阅及 `/tf` 变换树广播。
3.  **Docker 化**：提供标准的 Docker 镜像，简化环境部署。

### 第三阶段：评测系统与程序化生成 (P1 - High)
**目标**：提供标准化的算法评估能力和多样化的训练环境。
1.  **评测框架**：实现 PointNav、ObjectNav 等任务定义及 SR、SPL 等自动计算指标。
2.  **VLA/VLN 接口**：完善支持多模态输入的算法接口。
3.  **程序化场景**：实现基于规则的场景自动生成与复杂度评估工具。
4.  **资产导入**：支持 USD、GLB 等通用格式资产的直接导入。

### 第四阶段：虚实融合高级特性 (P2 - Medium)
**目标**：缩小虚实差距，支持高级研发需求。
1.  **场景重建**：实现 Gaussian Splatting/NeRF 数据的导入与渲染支持。*(优先级高于轨迹重放)*
2.  **轨迹重放**：实现真机数据的仿真回放功能。
3.  **模型标定**：实现基于真机数据的物理参数自动校准工具。
4.  **云端仿真**：支持在大规模集群上进行无头模式 (Headless) 的并行仿真训练。

---

## 5. 非功能性需求

*   **性能**：在单 GPU 上应支持至少 100+ 个并行环境的物理步进（不含高保真渲染），确保强化学习的训练效率。
*   **可用性**：所有核心功能模块需配备单元测试，核心流程需提供 Demo 示例代码。
*   **扩展性**：新增机器人或传感器类型时，无需修改核心引擎代码，仅需继承基类并添加配置即可。
*   **文档**：所有公开 API 需包含 Docstring（英文），并自动生成 API 文档。

---

## 6. 参考资料与依赖
*   **物理引擎**：[Genesis](https://github.com/Genesis-Embodied-AI/Genesis)
*   **ROS2 Bridge**：[Genesis ROS](https://github.com/Royalvice/genesis_ros) (humble branch)
*   **配置管理**：OmegaConf, Hydra
*   **文档工具**：Sphinx, MyST-Parser
